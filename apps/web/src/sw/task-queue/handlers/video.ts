/**
 * Video Generation Handler for Service Worker
 *
 * Handles video generation tasks with polling support.
 * ä½¿ç”¨é€šç”¨çš„åª’ä½“ç”Ÿæˆå·¥å…·å‡½æ•°æ¥å‡å°‘é‡å¤ä»£ç 
 */

import type {
  SWTask,
  TaskResult,
  HandlerConfig,
  TaskHandler,
} from '../types';
import { TaskExecutionPhase } from '../types';
import {
  mergeReferenceImages,
  pollVideoUntilComplete,
  fetchImageWithCache,
  parseSize,
  fitImageToCanvas,
} from '../utils/media-generation-utils';
import type { LLMReferenceImage } from '../llm-api-logger';

/**
 * Video generation response types
 */
interface VideoSubmitResponse {
  id: string;
  status: 'queued' | 'in_progress' | 'completed' | 'failed';
  progress: number;
  error?: string | { code: string; message: string };
}

/**
 * Submit response with log ID for tracking
 */
interface SubmitResult {
  response: VideoSubmitResponse;
  logId: string;
}

/**
 * Video generation handler
 */
export class VideoHandler implements TaskHandler {
  private abortControllers: Map<string, AbortController> = new Map();
  private pollingIntervals: Map<string, ReturnType<typeof setInterval>> = new Map();

  /**
   * Execute video generation task
   */
  async execute(task: SWTask, config: HandlerConfig): Promise<TaskResult> {
    const abortController = new AbortController();
    this.abortControllers.set(task.id, abortController);

    try {
      config.onProgress(task.id, 0, TaskExecutionPhase.SUBMITTING);

      // Submit video generation request
      const { response: submitResponse, logId } = await this.submitVideoGeneration(
        task,
        config,
        abortController.signal
      );

      // Notify remote ID
      config.onRemoteId(task.id, submitResponse.id);
      config.onProgress(task.id, 5, TaskExecutionPhase.POLLING);

      // Poll until completion using shared utility
      const result = await this.pollUntilComplete(
        submitResponse.id,
        task.id,
        config,
        abortController.signal,
        logId
      );

      return result;
    } finally {
      this.cleanup(task.id);
    }
  }

  /**
   * Resume video generation polling
   * æ¢å¤ä»»åŠ¡åªæ˜¯ç»§ç»­è½®è¯¢ï¼Œä¸åˆ›å»ºæ–°çš„ LLM API æ—¥å¿—æ¡ç›®
   * é€šè¿‡ taskId æŸ¥æ‰¾åŸå§‹æ—¥å¿—å¹¶åœ¨å®Œæˆæ—¶æ›´æ–°å®ƒ
   */
  async resume(task: SWTask, config: HandlerConfig): Promise<TaskResult> {
    if (!task.remoteId) {
      throw new Error('No remote ID for resume');
    }

    const abortController = new AbortController();
    this.abortControllers.set(task.id, abortController);

    // æŸ¥æ‰¾åŸå§‹ä»»åŠ¡çš„æ—¥å¿— IDï¼Œä»¥ä¾¿åœ¨è½®è¯¢å®Œæˆæ—¶æ›´æ–°å®ƒ
    const { findLatestLogByTaskId } = await import('../llm-api-logger');
    const originalLog = await findLatestLogByTaskId(task.id);
    const logId = originalLog?.id;

    try {
      config.onProgress(task.id, task.progress || 0, TaskExecutionPhase.POLLING);

      const result = await this.pollUntilComplete(
        task.remoteId,
        task.id,
        config,
        abortController.signal,
        logId // ä¼ é€’åŸå§‹æ—¥å¿— IDï¼Œè½®è¯¢å®Œæˆæ—¶ä¼šæ›´æ–°åŸå§‹æ—¥å¿—
      );

      return result;
    } finally {
      this.cleanup(task.id);
    }
  }

  /**
   * Cancel video generation
   */
  cancel(taskId: string): void {
    this.cleanup(taskId);
  }

  /**
   * Submit video generation request
   */
  private async submitVideoGeneration(
    task: SWTask,
    config: HandlerConfig,
    signal: AbortSignal
  ): Promise<SubmitResult> {
    const { videoConfig } = config;
    const { params } = task;

    // Build form data
    const formData = new FormData();
    formData.append('model', params.model || 'veo3');
    formData.append('prompt', params.prompt);

    if (params.duration) {
      formData.append('seconds', String(params.duration));
    }

    if (params.size) {
      formData.append('size', params.size);
    }

    // ä½¿ç”¨é€šç”¨å‡½æ•°åˆå¹¶å‚è€ƒå›¾ç‰‡
    const refUrls = mergeReferenceImages({
      referenceImages: params.referenceImages as string[] | undefined,
      uploadedImages: params.uploadedImages as any[] | undefined,
      inputReference: params.inputReference as string | undefined,
      inputReferences: params.inputReferences as any[] | undefined,
    });

    // å¤„ç†å‚è€ƒå›¾ç‰‡ï¼šè·å– Blobï¼Œè£å‰ªåˆ°ç›®æ ‡å°ºå¯¸ï¼Œç„¶åæ·»åŠ åˆ° FormData
    // åŒæ—¶æ”¶é›†è£å‰ªåçš„å›¾ç‰‡ä¿¡æ¯ç”¨äºæ—¥å¿—
    const { getImageInfo } = await import('../utils/media-generation-utils');
    const referenceImageInfos: LLMReferenceImage[] = [];
    
    if (refUrls.length > 0) {
      // è§£æç›®æ ‡å°ºå¯¸
      const targetSize = params.size ? parseSize(params.size as string) : null;

      for (let i = 0; i < refUrls.length; i++) {
        const url = refUrls[i];
        try {
          // ä½¿ç”¨é€šç”¨å‡½æ•°ä»ç¼“å­˜è·å–å›¾ç‰‡
          let blob = await fetchImageWithCache(url, signal);
          if (blob) {
            // å¦‚æœæŒ‡å®šäº†ç›®æ ‡å°ºå¯¸ï¼Œå°†å›¾ç‰‡é€‚é…åˆ°ç²¾ç¡®çš„ç›®æ ‡å°ºå¯¸ï¼ˆå±…ä¸­æ”¾ç½®ï¼Œä¸»è‰²è°ƒå¡«å……èƒŒæ™¯ï¼‰
            if (targetSize) {
              blob = await fitImageToCanvas(blob, targetSize.width, targetSize.height);
            }
            
            // è·å–å¤„ç†åçš„å›¾ç‰‡ä¿¡æ¯ç”¨äºæ—¥å¿—
            try {
              const info = await getImageInfo(blob, signal);
              referenceImageInfos.push({
                url: info.url,
                size: info.size,
                width: info.width,
                height: info.height,
              });
            } catch (err) {
              console.warn(`[VideoHandler] Failed to get processed image info for log`, err);
              referenceImageInfos.push({ url, size: blob.size, width: 0, height: 0 });
            }
            
            formData.append('input_reference', blob, `reference-${i + 1}.png`);
          } else {
            // ç¼“å­˜å’Œç½‘ç»œéƒ½å¤±è´¥æ—¶ï¼Œå›é€€åˆ°å‘é€ URL
            console.warn(`[VideoHandler] Failed to get reference image: ${url}`);
            formData.append('input_reference', url);
            referenceImageInfos.push({ url, size: 0, width: 0, height: 0 });
          }
        } catch (err) {
          console.warn(`[VideoHandler] Error fetching reference image: ${url}`, err);
          formData.append('input_reference', url);
          referenceImageInfos.push({ url, size: 0, width: 0, height: 0 });
        }
      }
    }

    // Import loggers
    const { debugFetch } = await import('../debug-fetch');
    const { startLLMApiLog, completeLLMApiLog, failLLMApiLog } = await import('../llm-api-logger');
    
    const startTime = Date.now();
    const model = (params.model as string) || 'veo3';
    
    // æ„å»ºè¯·æ±‚å‚æ•°çš„æ—¥å¿—è¡¨ç¤ºï¼ˆFormData æ— æ³•ç›´æ¥åºåˆ—åŒ–ï¼‰
    const requestParamsForLog = {
      model,
      prompt: params.prompt,
      ...(params.duration && { seconds: params.duration }),
      ...(params.size && { size: params.size }),
      ...(refUrls.length > 0 && { input_reference: `[${refUrls.length} images]` }),
    };
    
    const logId = startLLMApiLog({
      endpoint: '/videos',
      model,
      taskType: 'video',
      prompt: params.prompt as string,
      requestBody: JSON.stringify(requestParamsForLog, null, 2),
      hasReferenceImages: refUrls.length > 0,
      referenceImageCount: refUrls.length,
      referenceImages: referenceImageInfos,
      taskId: task.id,
    });

    // Use debugFetch for logging
    const response = await debugFetch(`${videoConfig.baseUrl}/videos`, {
      method: 'POST',
      headers: videoConfig.apiKey
        ? { Authorization: `Bearer ${videoConfig.apiKey}` }
        : undefined,
      body: formData,
      signal,
    }, {
      label: `ğŸ¬ æäº¤è§†é¢‘ç”Ÿæˆ (${model})`,
      logResponseBody: true,
    });

    if (!response.ok) {
      const errorText = await response.text();
      failLLMApiLog(logId, {
        httpStatus: response.status,
        duration: Date.now() - startTime,
        errorMessage: errorText,
        responseBody: errorText,
      });
      throw new Error(`Video submission failed: ${response.status} - ${errorText}`);
    }

    const data = await response.json();

    // è®°å½• remoteId åˆ°æ—¥å¿—ï¼Œä»¥ä¾¿åœ¨ SW é‡å¯æ—¶æ¢å¤
    if (data.id) {
      const { updateLLMApiLogMetadata } = await import('../llm-api-logger');
      updateLLMApiLogMetadata(logId, {
        remoteId: data.id,
        responseBody: JSON.stringify(data),
        httpStatus: response.status,
      });
    }

    // æ³¨æ„ï¼šè¿™é‡Œä¸è°ƒç”¨ completeLLMApiLogï¼Œå› ä¸ºè§†é¢‘è¿˜åœ¨å¼‚æ­¥ç”Ÿæˆä¸­
    // æœ€ç»ˆç»“æœä¼šåœ¨ pollUntilComplete å®Œæˆåæ›´æ–°

    return { response: data, logId };
  }

  /**
   * Poll video status until completion
   * ä½¿ç”¨é€šç”¨çš„è½®è¯¢é€»è¾‘ï¼Œä½†ä¿æŒä»»åŠ¡çº§åˆ«çš„è¿›åº¦å›è°ƒ
   */
  private async pollUntilComplete(
    videoId: string,
    taskId: string,
    config: HandlerConfig,
    signal: AbortSignal,
    logId?: string
  ): Promise<TaskResult> {
    const { videoConfig } = config;
    const startTime = Date.now();

    try {
      // ä½¿ç”¨é€šç”¨è½®è¯¢å‡½æ•°
      const result = await pollVideoUntilComplete(
        videoConfig.baseUrl,
        videoId,
        {
          onProgress: (progress, phase) => {
            config.onProgress(taskId, progress, phase);
          },
          signal,
          apiKey: videoConfig.apiKey,
          interval: 5000,
          maxAttempts: 1080, // 90 minutes
        }
      );

      const videoUrl = result.video_url || result.url;
      if (!videoUrl) {
        throw new Error('No video URL in completed response');
      }

      // æ›´æ–° LLM API æ—¥å¿—ï¼Œæ·»åŠ æœ€ç»ˆçš„è§†é¢‘ URL
      if (logId) {
        const { completeLLMApiLog } = await import('../llm-api-logger');
        completeLLMApiLog(logId, {
          httpStatus: 200,
          duration: Date.now() - startTime,
          resultType: 'video',
          resultCount: 1,
          resultUrl: videoUrl,
          responseBody: JSON.stringify(result),
        });
      }

      return {
        url: videoUrl,
        format: 'mp4',
        size: 0,
        width: result.width,
        height: result.height,
        duration: parseInt(result.seconds || '0') || 0,
      };
    } catch (error) {
      // æ›´æ–° LLM API æ—¥å¿—ï¼Œè®°å½•å¤±è´¥
      if (logId) {
        const { failLLMApiLog } = await import('../llm-api-logger');
        failLLMApiLog(logId, {
          duration: Date.now() - startTime,
          errorMessage: error instanceof Error ? error.message : String(error),
        });
      }
      throw error;
    }
  }

  /**
   * Cleanup resources
   */
  private cleanup(taskId: string): void {
    const controller = this.abortControllers.get(taskId);
    if (controller) {
      controller.abort();
      this.abortControllers.delete(taskId);
    }

    const interval = this.pollingIntervals.get(taskId);
    if (interval) {
      clearTimeout(interval);
      this.pollingIntervals.delete(taskId);
    }
  }
}
